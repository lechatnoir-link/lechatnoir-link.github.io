<!DOCTYPE html><html lang="zh-CN" id="theme-light-mode"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="abel-neko"><title>Literature Curation Plan · le chat-noir's blog</title><meta name="description" content="Reorganize and Record the top-level papers I have read.




Important#T000
Attention is All you Need. (#T001 Transformer)
BERT: Pre-training of Deep B"><meta name="keywords" content="Blog, Hexo"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="shortcut icon" type="image/x-icon" href="/images/abelsblog.png"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/insight.css"><link rel="stylesheet" href="/css/search.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><script src="/js/jquery.js"></script><meta name="generator" content="Hexo 5.4.2"></head><body><div class="page-top animated fadeInDown"><div class="nav"><li> <a href="/">Home</a></li><li> <a href="/archives">Archives</a></li><li> <a href="/tags">Tags</a></li><li> <a href="/about">About</a></li><li> <a href="/links">Links</a></li></div><div class="information"><div class="nav_right_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li><li><a class="fa fa-search" onclick="openWindow();"></a></li></div><div class="avatar"><img src="/images/avatar.jpg"></div></div></div><div class="sidebar animated fadeInDown"><div class="sidebar-top"><div class="logo-title"><div class="title"><img src="/images/avatar.jpg" style="width:220px;" alt="favicon"><h3 title=""><a href="/">le chat-noir's blog</a></h3><div class="description"><p>abel's personal blog, recording his research life.</p></div></div><ul class="social-links"><li><a target="_blank" rel="noopener" href="https://github.com/abel-nlp"><i class="fa fa-github"></i></a></li><li><a href="mailto:abel.nlp@outlook.com"><i class="fa fa-envelope"></i></a></li></ul></div></div><div class="footer"><div class="p"> <span> 2019-2023 copyright </span><i class="fa fa-star"></i><span> abel-neko</span></div><div class="by_farbox"><span>Powered by Hexo </span><span> & </span><span>abel.nlp </span></div><div class="beian"></div></div></div><div class="main"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>Literature Curation Plan</a></h3></div><div class="post-content"><p><p>Reorganize and Record the top-level papers I have read.</p>
<span id="more"></span>



<h1 id="Important"><a href="#Important" class="headerlink" title="Important"></a>Important</h1><h2 id="T000"><a href="#T000" class="headerlink" title="#T000"></a>#T000</h2><ul>
<li>Attention is All you Need. (#T001 Transformer)</li>
<li>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. (#T002 BERT)</li>
<li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. (#T003 CoT)</li>
<li>Augmenting Reinforcement Learning with Human Feedback. (#T004 RLHF)</li>
<li>Toolformer: Language Models Can Teach Themselves to Use Tools. (#T005 Toolformer)</li>
<li>SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient. (#T006 SWARM)</li>
<li>RWKV: Reinventing RNNs for the Transformer Era. (#T007 RWKV)</li>
<li>LIMA: Less Is More for Alignment. (#T008 LIMA)</li>
<li>ZeRO: Memory Optimizations Toward Training Trillion Parameter Models. (#T009 ZoRO)</li>
</ul>
<h1 id="Survey"><a href="#Survey" class="headerlink" title="Survey"></a>Survey</h1><h2 id="S000"><a href="#S000" class="headerlink" title="#S000"></a>#S000</h2><ul>
<li>Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing.  (#S001 Prompt)</li>
</ul>
<h1 id="Report"><a href="#Report" class="headerlink" title="Report"></a>Report</h1><h2 id="R000"><a href="#R000" class="headerlink" title="#R000"></a>#R000</h2><ul>
<li><p>Improving Language Understanding by Generative Pre-Training. (#R001 GPT)</p>
</li>
<li><p>Language Models are Unsupervised Multitask Learners. (#R002 GPT2)</p>
</li>
<li><p>Language Models are Few-Shot Learners. (#R003 GPT3)</p>
</li>
<li><p>Training language models to follow instructions with human feedback. (#R004 InstructGPT)</p>
</li>
<li><p>GPT-4 Technical Report. (#R005 GPT4-Report1)</p>
</li>
<li><p>Sparks of Artificial General Intelligence: Early experiments with GPT-4. (#R006 GPT4-Report2)</p>
</li>
</ul>
<h1 id="RE"><a href="#RE" class="headerlink" title="RE"></a>RE</h1><h2 id="E000"><a href="#E000" class="headerlink" title="#E000"></a>#E000</h2><h2 id="Joint"><a href="#Joint" class="headerlink" title="Joint"></a>Joint</h2><ul>
<li>TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking. (#E001 TPLinker)</li>
<li>A Novel Cascade Binary Tagging Framework for Relational Triple Extraction. (#E002 CasRel)</li>
<li>A Frustratingly Easy Approach for Entity and Relation Extraction. (#E003 PURE)</li>
<li>A Novel Global Feature-Oriented Relational Triple Extraction Model based on Table Filling. (#E004 GRTE)</li>
<li>PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction. (#E005 PRGC)</li>
<li>OneRel:Joint Entity and Relation Extraction with One Module in One Step. (#E006 OneRel)</li>
<li>RFBFN: A Relation-First Blank Filling Network for Joint Relational Triple Extraction. (#E007 RFBFN)</li>
<li>UniRel: Unified Representation and Interaction for Joint Relational Triple Extraction. (#E008 UniRel)</li>
</ul>
<h2 id="Few-shot"><a href="#Few-shot" class="headerlink" title="Few-shot"></a>Few-shot</h2><ul>
<li><p>FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation. (FewRel)</p>
<p><strong>AND</strong> :</p>
<p>FewRel 2.0: Towards More Challenging Few-Shot Relation Classification. (FewRel2.0)</p>
<p>(#E009 FewRel)</p>
</li>
<li><p>Few-Shot Relational Triple Extraction with Perspective Transfer Network. (#E010 PTN)</p>
</li>
<li><p>Query-based Instance Discrimination Network for Relational Triple Extraction. (#E011 QIDN)</p>
</li>
<li><p>Relation-Guided Few-Shot Relational Triple Extraction. (#E012 RelATE)</p>
</li>
</ul>
<h1 id="NER"><a href="#NER" class="headerlink" title="NER"></a>NER</h1><h2 id="N000"><a href="#N000" class="headerlink" title="#N000"></a>#N000</h2><h2 id="Discontinuous"><a href="#Discontinuous" class="headerlink" title="Discontinuous"></a>Discontinuous</h2><ul>
<li>Unified Named Entity Recognition as Word-Word Relation Classification. (#N001 W2NER)</li>
<li>Rethinking Boundaries: End-To-End Recognition of Discontinuous Mentions with Pointer Networks. (#N002 MAPtr)</li>
<li>Discontinuous Named Entity Recognition as Maximal Clique Discovery. (#N003 Mac)</li>
</ul>
<h1 id="KGE"><a href="#KGE" class="headerlink" title="KGE"></a>KGE</h1><h2 id="K000"><a href="#K000" class="headerlink" title="#K000"></a>#K000</h2><h1 id="MRC"><a href="#MRC" class="headerlink" title="MRC"></a>MRC</h1><h2 id="M000"><a href="#M000" class="headerlink" title="#M000"></a>#M000</h2><h1 id="CV"><a href="#CV" class="headerlink" title="CV"></a>CV</h1><h2 id="C000"><a href="#C000" class="headerlink" title="#C000"></a>#C000</h2><ul>
<li>An Image Is Worth 16X16 Words: Transformers for Image Recognition at scale.  (#C001 ViT)</li>
<li>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. (#C002 Swin-Transformer)</li>
<li>Learning Transferable Visual Models From Natural Language Supervision. (#C003 CLIP)</li>
<li>BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. (#C004 BLIP)</li>
</ul>
<p><em>Content will be continuously added.</em></p>
</p><div class="tip">本文著作权归作者所有 <br>Personal blog, please do not repost.<br>Author: abel-neko</div></div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2023-05-02</span><i class="fa fa-tag"></i><a class="tag" href="/tags/todo-list/" title="todo-list">todo-list </a><span class="leancloud_visitors"></span><span>About 447 words, 1 min 29 sec  read</span></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="" onclick="javascript:join_favorite()" ref="sidebar"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/intent/tweet?text=I%20have%20found%20a%20great%20blog.%0A%0Ale%20chat-noir's%20blog%20%C2%B7%20Literature%20Curation%20Plan%0Ahttps://lechatnoir-link.github.io/2023/05/02/Literature-Curation-Plan/%0A"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2023/05/02/W2NER/" title="W2NER">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2023/05/01/DeepLearning-Environment-Setting/" title="DeepLearning Environment Setting">Next</a></li></ul></div><script src="/js/visitors.js"></script><a id="comments"></a><div id="vcomments" style="margin:0 30px;"></div><script src="//cdn.jsdelivr.net/gh/xcss/valine@latest/dist/Valine.min.js"></script><script src="//cdnjs.loli.net/ajax/libs/jquery/3.2.1/jquery.min.js"></script><script>var valine = new Valine({
  el:'#vcomments',
  notify:false || false, 
  verify:false|| false, 
  app_id:'AMhfBb1KUFMvBJpO3kpEzMsf-gzGzoHsz',
  app_key:'cAAsBguwh3FhDlERQ4KkGkQJ',
  placeholder:'write something?',
  path: window.location.pathname,
  serverURLs: 'https://amhfbb1k.lc-cn-n1-shared.com',
  visitor:true,
  recordIP:true,
  avatar:'monsterid',
  highlight: true,
  avatarForce: true,
  enableQQ: true,
  requiredFields: ['nick']
})
</script></div></div></div></div><script src="/js/darkLightToggle.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/add-bookmark.js"></script><script>(function(window){var INSIGHT_CONFIG={TRANSLATION:{POSTS:"Posts",PAGES:"Pages",CATEGORIES:"Categories",TAGS:"Tags",UNTITLED:"(Untitled)",},CONTENT_URL:"/content.json",};window.INSIGHT_CONFIG=INSIGHT_CONFIG})(window);</script><script src="/js/insight.js" defer></script><div class="searchbox ins-search"><div class="searchbox-container ins-search-container"><div class="searchbox-input-wrapper"><input class="searchbox-input ins-search-input" type="text" placeholder="Search..."><span class="searchbox-close"><a class="fa fa-times-circle" onclick="closeWindow();"></a></span></div><div class="searchbox-result-wrapper ins-section-wrapper"><div class="ins-section-container"><p>Seraching...</p></div></div></div></div></body></html>