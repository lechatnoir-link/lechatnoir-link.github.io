{"pages":[{"title":"links","text":"なに，(⊙o⊙)？ ame","link":"/links/index.html"},{"title":"about","text":"Hello! I’m abel. I’m a Master’s student in Computer Science and Technology now. My research focuses on Information Extraction, Knowledge Graph in Natural Language Processing. My research skill is still relatively low now.😅 Thank you for visiting. I have created personal blogs many times before, by myself or using frameworks made by others such as Wordpress, Typecho and Jekyll; However, due to my blind pursuit of dynamic effects, the blog was designed more and more fancy, but the content was more and more empty, and even once the blog was dressed up as a personal love memorial website, making the blog stupid； After a year, I re-build my blog with the encouragement of my friend, the following provisions are made: Only three types of articles will be shared: Summaries of literature in my own research area and related fields; Summaries of techniques accumulated in the course of research, projects and personal practice; Non-regular personal summaries based on todo-list. In the three types of articles: The summary of the literature is required to be understandable and rewarding for those who have relevant research experience. Aka. quality assurance, preferring lack to lack; Technical summaries that I can read and understand, or I can quickly remember when I needs them, but that do not require that others understand the exact meaning; Personal summary requirements to todo-list-based. The blog can be revamped after a certain level of academic or technical achievement in order to “get the details right”; No more fancy plugins, features and pages on top of the current one; I strive to make this blog an accumulative testimony of personal thoughts and techniques, and if I’m busy or have nothing good to offer, I can break off, and perfunctory work is strictly prohibited. Contact me: Github Email","link":"/about/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"DeepLearning Environment Setting","text":"How to elegantly build a deep learning environment and debug and run your own programs locally using the resources of a remote server. Prerequisites 搭载了显卡和conda环境的服务器，服务器可以联网(能conda、pip及wget)； 自己在服务器的账号引入了conda和cuda的环境变量 本地下载了PyCharm或VSCode； 拥有服务器管理员权限或者与管理员沟通过开放端口(只限启用jupyter才需要) 基本运行环境创建Conda环境创建 登录自己服务器账号后，需要创建所需的虚拟环境： 1234conda create -n env_name python=3.7# 自己指定python版本conda remove -n env_name --all# 如果以后需要删除环境，则可以使用该命令 激活虚拟环境： 1234567conda activate env_name# 或者source activate env_name# 关闭环境：conda deactivate# 或source deactivate 安装自己所需要的第三方库： 1234pip3 install package_name# 或者临时使用清华源pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple package_name# 或者使用conda安装，自行搜索 PyCharm配置 新建项目，为了方便，最好保持本地项目和服务器所需要配置的目录名一样； 在新建项目处，Location处是本地的项目路径。 选择Preciously的解释器，并点击Add Interpreter，选择SSH; 如果本地已经在某个服务器上已经创建过解释器，则直接在Existing处选择即可，否则，依旧点此处，再点击...处进入SSH Configurations页面；点击+，输入服务器地址、用户名和密码，之后再OK–&gt;Next: 如果是第一次添加，则可能出现下图的情况，直接点击Move，再点Next按照提示操作 如下图，选择Existing，点击...，之后会出现一个选择路径的选项框，按照自己账户所存在的根目录(如home或者data)，在自己账号下面，逐步点击.conda–&gt;envs–&gt;需要的虚拟环境–&gt;bin–&gt;python3即可，选择OK和Create，按照提示进入项目中。 之后，选择Tools–&gt;Deployment–&gt;Configuration...；一般来说，现在已经有了SFTP的选项，因为刚刚创建SSH解释器时，这里也同时附带被创建了； 类似于选择.conda的操作，选择好Local path和服务器Deployment path，即后续项目代码同步的路径； 如果有需要排除同步的路径，例如模型本身或者较大的数据集，则可以在Excluded Paths中选好本地及服务器不同步的路径； 完成这些配置后，此时是默认不自动同步的，因此可以进入Tools–&gt;Deployment–&gt;Options..，将Upload changed files automatically to the default server改成On explicit save action，即自己按Ctrl+S时进行同步，当然也可以改成Always; 之后，在PyCharm的右下角，将&lt;no default server&gt;改成上面配置好的SFTP； 大功告成。 VSCode配置 VSCode的配置相对比较简单，因此这里中简述基本步骤，不做相信说明，有需要可自行网上检索 下载微软官方插件Remote - SSH； 在远程资源管理器中的右上角的小齿轮中，输入： 12345Host &lt;远程主机名称&gt; HostName &lt;远程主机IP&gt; User &lt;用户名&gt; Port &lt;ssh端口，默认22&gt; IdentityFile &lt;本机SSH私钥路径&gt; Host ：连接的主机名称，可自定义； Hostname ：远程主机的 IP 地址； User ：用于登录远程主机的用户名； Port ：用于登录远程主机的端口，SSH 默认为 22 ； IdentityFile ：本地的私钥文件 id_rsa 路径； 一开始是没有私钥文件的，需要使用以下方式得到： 本地： 12cd ~/.ssh# 复制 id_rsa.pub的内容 服务器： 1234cd ~/.sshvim authorized_keys# 然后将刚刚复制的文件粘贴进去# 若不熟悉vim请自行检索 之后，本地的id_rsa即为私钥 小齿轮还可以再新增其他服务器的或者其他账户的信息； 需要注意的问题： 创建好后，左下角可以选择连接服务器，连接后需要下载相应的插件，如python和jupyter相关； 有时候vscode的网络不好，连接服务器下载会非常慢，插件也是如此； 如果难以下载，可以本地下载好，包括服务器本身或者需要按照的插件，然后进入服务器的.vscode-server中进行配置，具体自行查询 配置远程Jupyter虽然使用debug也非常方便，但是有时候还是希望可以利用Jupyter的cell执行特点来执行代码。 因此，先在虚拟环境中pip install jupyter； 假设服务器有比较严格的防火墙，那么请提前确定好端口(假设是4399)，让管理员开启： 12sudo firewall-cmd --zone=public --add-port=4399/tcp --permanentsudo firewall-cmd --reload 之后，初始化jupyter配置： 产生配置文件： 1jupyter notebook --generate-config 设置密码： 1jupyter notebook password 复制密钥： 123cd ~/.jupytervim jupyter_notebook_config.json# 将password的value复制下来 配置端口： 12vim jupyter_notebook_config.py# 拉到最后 1234c.NotebookApp.ip = '*'c.NotebookApp.password = &quot;刚刚复制的密钥&quot;c.NotebookApp.open_browser = Falsec.NotebookApp.port = 4399 启动jupyter： 123jupyter notebook# 然后测试一下，例如浏览器输入 http://浏览器ip:4399# 输入token密码 注意，在哪里启动jupyter，那么其根目录就在哪里； 长期挂载： 1nohup jupyter notebook &gt; note.log &amp; 则会一直挂在后台，保持运行 在PyCharm中使用jupyter： 在项目中新建一个jupyter文件，打开后右上角设置其configuration: 选中Configured Server，输入http://xxx.xxx.xxx.xxx:4399，然后回到文件运行代码，运行时会提示输入密码，输入即可; 大功告成！ 后续若有新增，将会于此补充","link":"/2023/05/01/DeepLearning-Environment-Setting/"},{"title":"Literature Curation Plan","text":"Reorganize and Record the top-level papers I have read. Important #T000 Attention is All you Need. (#T001 Transformer) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. (#T002 BERT) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. (#T003 CoT) Augmenting Reinforcement Learning with Human Feedback. (#T004 RLHF) Toolformer: Language Models Can Teach Themselves to Use Tools. (#T005 Toolformer) SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient. (#T006 SWARM) RWKV: Reinventing RNNs for the Transformer Era. (#T007 RWKV) LIMA: Less Is More for Alignment. (#T008 LIMA) ZeRO: Memory Optimizations Toward Training Trillion Parameter Models. (#T009 ZoRO) Survey #S000 Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. (#S001 Prompt) Report #R000 Improving Language Understanding by Generative Pre-Training. (#R001 GPT) Language Models are Unsupervised Multitask Learners. (#R002 GPT2) Language Models are Few-Shot Learners. (#R003 GPT3) Training language models to follow instructions with human feedback. (#R004 InstructGPT) GPT-4 Technical Report. (#R005 GPT4-Report1) Sparks of Artificial General Intelligence: Early experiments with GPT-4. (#R006 GPT4-Report2) RE #E000 Joint TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking. (#E001 TPLinker) A Novel Cascade Binary Tagging Framework for Relational Triple Extraction. (#E002 CasRel) A Frustratingly Easy Approach for Entity and Relation Extraction. (#E003 PURE) A Novel Global Feature-Oriented Relational Triple Extraction Model based on Table Filling. (#E004 GRTE) PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction. (#E005 PRGC) OneRel:Joint Entity and Relation Extraction with One Module in One Step. (#E006 OneRel) RFBFN: A Relation-First Blank Filling Network for Joint Relational Triple Extraction. (#E007 RFBFN) UniRel: Unified Representation and Interaction for Joint Relational Triple Extraction. (#E008 UniRel) Few-shot FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation. (FewRel) AND : FewRel 2.0: Towards More Challenging Few-Shot Relation Classification. (FewRel2.0) (#E009 FewRel) Few-Shot Relational Triple Extraction with Perspective Transfer Network. (#E010 PTN) Query-based Instance Discrimination Network for Relational Triple Extraction. (#E011 QIDN) Relation-Guided Few-Shot Relational Triple Extraction. (#E012 RelATE) NER #N000 Discontinuous Unified Named Entity Recognition as Word-Word Relation Classification. (#N001 W2NER) Rethinking Boundaries: End-To-End Recognition of Discontinuous Mentions with Pointer Networks. (#N002 MAPtr) Discontinuous Named Entity Recognition as Maximal Clique Discovery. (#N003 Mac) KGE #K000 MRC #M000 MM #C000 An Image Is Worth 16X16 Words: Transformers for Image Recognition at scale. (#C001 ViT) Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. (#C002 Swin-Transformer) Learning Transferable Visual Models From Natural Language Supervision. (#C003 CLIP) BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. (#C004 BLIP) A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation. (#C005 NMT) Multi-modal Graph Fusion for Named Entity Recognition with Targeted Visual Guidance. (#C006 UMGF) Good Visual Guidance Makes A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction. (#C007 HVPNeT) Content will be continuously added.","link":"/2023/05/02/Literature-Curation-Plan/"},{"title":"Paper Notes Collection One","text":"Paper Notes Collection for Survey and Large Language Model. 论文笔记合集：对部分论文做主要内容概括 CoTNotes for Paper: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models Background本文主要进行CoT评估的任务分别为： Arithmetic Reasoning：数学推理，即图1中所示的数学问题； Commonsense Reasoning：常识推理； Symbolic Reasoning：符号推理。例如要求将出现单词的首字母或者尾字母拼接在一起，虽然人类很容易解决该问题，但对模型而言非常具有挑战性。 Motivation思维链(CoT)在人类思考活动中很常见。当我们思考问题的时候，往往不是直接得到答案，而是将问题分解，然后逐步向正确答案靠近。 类似地，以CoT方式对模型进行提示，理论上也能得到比较好的结果，这是因为很多数据集在训练时，直接给出问题和答案，然后然后让模型去学习，但是为什么会得到这个答案，模型可能并不了解，而引入CoT后，这些中间步骤会极大丰富模型学会为什么得到此答案的理由。 如图1所示： 传统的训练方法中，对于一个问题，直接给出答案，模型难以学到得到这个答案的具体原因； 而在CoT提示方法中，给出的不仅是答案，而是增加了得到这个答案的中间步骤(也就是思考的过程)，通过这种方式，引导模型在解决类似问题时，也会先生成中间步骤，再得到最终答案，以提高准确性。 本文发现，单纯增大模型的规模，不足以在一些具有挑战性的任务上提升对应的性能，比如上面提到的三个问题。 因此，本文通过两个简单的思想，探索了大模型的在不扩大规模的前提下，如何提高模型在这些推理问题上的性能。这两个思想主要是：以前的大量语料和参数量已经给了模型产生中间步骤的能力；通过提示的方式可以进行few-shot学习，而无需微调。 具体来说，就是人工设置每种任务类型的CoT提示，作为few-shot的学习示例，这里的图2以数学推理为例： 可以看到，对于标准(传统)提示而言，随着模型规模的增加，性能的确有上升；但要想达到监督模型所得到性能表现还有些困难，并且训练大规模的语言模型，所耗费的资源是很多的。 而在这些模型上，仅仅通过增加CoT的提示，便有了达到甚至超过监督模型的性能。 同时也能看出，CoT提示在规模比较大的模型上表现的更好，也许说明了，模型的规模越大，越有利于产生中间结果，越利于进行few-shot学习，再配合上合适的提示，大模型的性能才能被更好地被发挥出来。 ToolformerNotes for Paper: Toolformer: Language Models Can Teach Themselves to Use Tools Motivation大语言模型(LLM)虽然在few-shot和zero-shot方面实现了非常好的提升，并通过参数规模、语料增加而展现了其“涌现”的特点，但这些模型依旧存在一些固有的限制，例如：从最近的事件中获取最新的信息；精确的数学计算；理解低资源语言；缺乏对时间进程的感知等。 但我们知道在日常生活中，早就有相关的工具能够很好的解决这些问题，那就是各种实用工具，比如搜索引擎、计算器和日历等。如果让大语言模型能够学会如何正确地使用这些工具，而不是寄希望于让他们自己解决所有问题，将极大节省训练的花费。为此，本文提出了Tooformer，以让模型拥有使用外部工具的能力，他们的方法主要有以下几个特点： 要能以自监督的方式学习，因为大量的人工标注是昂贵的；此外，人类认为有用的信息，对模型而言则不一定，因此让模型自己学习或许更有益； 语言模型不应该失去它的通用性，应该能够自己决定何时、如何使用哪种工具。与现有的方法相比，这使得对工具的使用更加全面，不受特定任务的束缚。 文中的调用方式为： 分别表示只有调用本身和一个调用包含其结果。下图的示例就是一个调用(c)的工具(a)，输入(i)和结果(r)。 作者们构建这种使用外部工具的模型的主要方法大致为： 首先让语言模型自己对大量的数据集按照自己的方式进行可能的API调用标注(因为现有的人工写的好的API调用例子并不多)； 然后，再利用自监督损失来确定哪些API调用切实有助于模型的预测； 最后，利用这些有用的API注释来微调模型。 如图所示： 首先对于输入文本，先让语言模型利用其上下文学习能力去生成大量可能的API调用示例，再实际去执行这些API调用，然后用空序列调用做对比进行自监督损失以选出更可能有效的API调用，最后再利用这些API进行微调。 本文主要使用了以下几种工具，利用GPT-J(6B)做微调的模型，实验结果的确有效，很多数据集上甚至比OPT(66B)和GPT3(175B)高得多：1). 问答；2). 计算器；3). 维基百科搜索；4). 机器翻译；5). 日历。 RLHFInstructGPT","link":"/2023/05/10/Paper-Notes-Collection-One/"},{"title":"Server Management Notes","text":"A summary of some common commands that are used to manage servers. 管理员账户相关 root Ubuntu默认是没有root的，而是以sudo用户来代替，这种方式在绝大多数时候是安全可用的，但当sudo用户有操作不当时，会导致系统出现无法修复的问题，因此在有这种需要时，可以提前设置root用户。 在具有sudo权限的用户下进行操作； 设置root账户密码： 1passwd root 编辑配置文件： 1234sudo vim /etc/ssh/sshd_config# 然后输入以下命令：PermitRootLogin yesPasswordAuthentication yes 重启ssh服务： 1systemctl restart ssh 需要注意，root用户具有完全的权限，比一般的sudo用户更高，使用时务必小心。 sudo 在某个管理员账户下，给某个用户分配sudo权限，一种简单的方式是将其添加到sudo的组里面； 查看sudo用户： 12345678# 查看sudo用户有哪些# 先安装一个包sudo apt-get install members# 再查看members sudo# 或者在某个用户的终端下输入groupsgroups # 以查看该用户当前所属的组 将用户添加到sudo组： 12sudo usermod -aG sudo username# 将username替换为用户账户名 将用户从sudo组移除： 1sudo deluser username sudo 网络配置相关 参考: asimok’s blog 端口防火墙： 打开某个端口的防火墙 12sudo firewall-cmd --zone=public --add-port=4399/tcp --permanentsudo firewall-cmd --reload 查看所有打开的端口 123sudo firewall-cmd --zone=public --list-ports# 或者限定端口的开放协议 如tcpsudo firewall-cmd --zone=public --list-ports tcp 配置ipv6： 检查是否已经启用ipv6支持 1sudo cat /proc/net/if_inet6 如果结果不为空，直接下一步，否则： 12345678sudo vim /etc/sysctl.conf# 添加以下内容:net.ipv6.conf.all.disable_ipv6 = 0 net.ipv6.conf.default.disable_ipv6 = 0# 之后，执行：sudo sysctl -p# 检查是否启用：sudo cat /proc/net/if_inet6 先找一个比较快的ipv6的DNS，比如清华源等； 修改配置文件，添加DNS: 123sudo vim /etc/systemd/resolved.conf# 添加DNS，比如：DNS=2001:67c:2b0::6 2001:67c:2b0::4 重启DNS服务： 12sudo systemctl restart systemd-resolved sudo systemctl enable systemd-resolved 启动配置文件： 123sudo mv /etc/resolv.conf /etc/resolv.conf.bak# 先将原来的文件备份sudo ln -s /run/systemd/resolve/resolv.conf /etc/ 检查是否启用成功： 1sudo cat /etc/resolv.conf 其他 利用Docker配置私人网盘 首先拉取docker: 1docker pull cloudreve/cloudreve 接着创建必要的文件： 123mkdir -vp cloudreve/{uploads,avatar} \\&amp;&amp; touch cloudreve/conf.ini \\&amp;&amp; touch cloudreve/cloudreve.db 然后启动docker： 先获取刚刚创建文件的路径：pwd，假设返回的路径是: /data0/driver 然后配置文件，并启动： 123456789sudo docker run -d \\--name docker-image-name \\-p 5212:5212 \\--mount type=bind,source=/data0/driver/cloudreve/conf.ini,target=/cloudreve/conf.ini \\--mount type=bind,source=/data0/driver/cloudreve/cloudreve.db,target=/cloudreve/cloudreve.db \\-v /data0/driver/cloudreve/uploads:/cloudreve/uploads \\-v /data0/driver/cloudreve/avatar:/cloudreve/avatar \\-e TZ=&quot;Asia/Shanghai&quot; \\cloudreve/cloudreve:latest 在新版的cloudreve中，查看docker日志是没有初始管理员密码的，因此要进入docker里面重置： 1docker exec -it docker-image-name ./cloudreve --database-script ResetAdminPassword 即可查看到到新的初始密码，初始账户为: admin@cloudreve.org 常用docker命令： 1234docker ps # 查看运行中容器docker stop xxxxdocker rm -f xxxxdocker restart xxxx","link":"/2023/05/11/Server-Management-Notes/"},{"title":"Todo-List 2023 Spring","text":"Basic schedule from May to September 2023 Jun2nd Week 整理并发布至少一篇文献相关博客 发布大模型校园内测版 整理并发布至少一个类型的LeetCode刷题总结 1st Week 持续进行科研任务 重新开始阅读和整理文献 完成前期LeetCode题的整理 May5th Week 完成大模型的训练部署 完成分布式系统的期末考试 正式进入科研+刷题阶段 4th Week 整理和打印分布式系统各项资料 复习分布式系统，完成应试准备 基本完成论文代码的修改和运行测验 基本完成大模型的数据工作、项目的模型收尾工作 3rd Week这一周真的光玩塞尔达了！！但是太好玩啦！ 塞尔达-王国之泪！！！！！！！！ 2nd Week 完成W2NER的论文笔记 完成CoT的论文笔记(in 论文合集1) 完成实习项目算法部分的余下全部内容，并完善和修改以前部署的部分 项目交接 1st Week 完成分布式系统的课件整理和概览 完成非科研和课业内容的前期杂事 完成第二版代码的修改和验证性训练","link":"/2023/05/02/Todo-List-2023-Spring/"},{"title":"W2NER","text":"Notes for Paper: Unified Named Entity Recognition as Word-Word Relation Classification 论文笔记主要记录和分析论文动机和思想，不对具体方法的细节和实验做讨论 Background:现有的NER问题可以大致分成三种： 简单实体(flat)，实体的构成比较简单，只识别出实体的开始和结束位置即可； 重叠(嵌套)实体(overlapped)，会出现多个实体包含相同的token的情况； 不连续实体(discontinuous)，实体由位置上不相邻的token构成。 如图1所示，在(a)中的实体$e_1$就是简单实体，而$e_2$则是不连续实体。又因为这两个实体同时出现在同一个句子中，并且有相互重叠的部分，即aching in，因此它们又是重叠的实体。 现有的NER方法大致可分为四种： 序列标注方法，简单来讲就是对每个token分配一个标签，以识别每个token在一个实体中扮演的角色。如图1中的$e_1$，我们可以将aching in legs分别标为BIE，以表示他们为开始、中间和结束。虽然这种方式简单直观，但是缺陷也很明显，即出现重叠和非连续实体时，简单的标签就无法完成任务了，并且还需要仔细地设计多种标签，复杂度非常高，也不利于解码； 基于超图的方法，既然标注法对一个token只能分配一个标签，那么利用节点和边的特点(一个节点可以有多个边)来表示所有的实体span，在一定程度上缓解了标注法的实体嵌套问题，但推理时会受到虚假结构和结构模糊性问题的影响(原文如此说，因为还没读过相关方法的论文，没有理解，暂时划掉)； 序列生成的方法：既然标注很麻烦，那干脆利用Seq2Seq的方式，直接生成实体，这样会不受嵌套和不连续的影响，但是会受到解码效率和偏差暴露的影响； 基于span的方法：一般可以列举所有可能的span，然后对span进行分类，但这种方式不仅会受到span的长度限制，还会因为枚举造成大量的资源消耗。 Motivation：本文认为，上述方法的核心其实还是在寻找实体的边界，这种思想也许在解决某一个具体问题上有效，但如果想同时解决三种实体识别的问题，也就是建立一个统一的NER模型，那就不能仅仅只看实体的边界了。 因此，本文认为这种统一模型的主要瓶颈在于如何建模好单词之间的相邻关系，因为只确定边界只是确定了实体的大致范围，至于词之间的关系：是复用的还是不相邻的？需要用其他方式来表示。 所以本文提出了自己的方法W2NER，该方法主要对词之间的两类关系，准确地说是三类关系进行建模，即： None:无关系； NNW(Next-Neighboring-Word)：下一个邻接词； THW-*:(Tail-Head-Word-*): 头尾词，*表示实体类型 简单来说，THW确定了所有可能的实体边界，NNW确定了实体边界里面的各词之间的关系。如图2所示: THW-S确定了两个S(Symptom)类型的实体范围，(从尾找到头)：aching in legs 和 aching in legs and shoulders； NNW确定了词之间的关系，即当前单词的下一个词是谁，可以看到in和and并没有NNW关系，所以在两个实体范围中，只能解码出:aching in legs和aching in shoulders两个实体，而这样恰好解决了不连续的问题。 这种思想也可以结合图2和图1(b)来直观地感受。 Model and Experiment:本文使用了Bert和LSTM作为编码器，利用卷积层提取词之间的表示，最后利用双仿射和多层感知机联合分类出词之间的关系。模型总体结构如图3所示： 使用卷积层的目的非常直观，因为本文建模的方式就是表的形式，而CNN相对也很适合处理这种结构的表示。在卷积层中，使用了条件层归一化操作(Conditional Layer Normalization)，论文认为这样能够有效产生词对的网格(表)的表示；之后利用类似Bert的方式，增加了词的位置信息和表格的区域信息；最后，使用不同的空洞卷积以捕获不同词距离之间的交互信息。 在完成上面对表格的表示refine后，论文使用联合的预测器进行最后的token标记分类。因为原文提到先前的工作验证了MLP和biaffine联合使用有利于关系分类。 具体的实验内容可见原文，这里以ShARe14数据集为例，如下图，可见W2NER模型在重叠和不连续场景长的确都取得了明显的性能提升。 Decode Strategy:解码的基本思想是利用词之间的关系来确定词和词之间的路径，文中以四个示例来展示解码的具体操作： 需要注意的是，图中的下方文字，划线的是具体的实体，大写字母代表了实体中的词。而图中的蓝线表示NNW关系，红色的线表示THW关系。 有两个实体AB和DE，属于简单实体，因此直接就能解码出来； 有重叠的实体：ABC和BC，但因为ABC和BC均有THW关系，因此也可以解码除了； 有重叠和不连续的实体：ABC和ABD，除了利用THW关系来解决重叠问题外，NNW也从B直接关联到D，从而识别出了不连续的ABD实体； 比较复杂的实体：ACD和BCE，和上面不同的在于，有可能出现ACE和BCD的路径，但是通过THW的限制，使得这两种情况被排除。 Thinking: 本文的动机、对应解决方法和解码策略都非常自然，需要注意的是： 并非所有表格表示的东西都适合CNN，如果token的分类和其他token没有太多直接的关系，那么使用CNN不一定会有正向的作用；当然，本文中因为NNW关系本就需要邻近token的信息，所以非常适合，但是核不易太大； 这种方法可以迁移到关系提取(联合提取)上，但是重叠的类型会更多，并且会引入超出实体级别的关系。如果依旧保留NNW这样的关系，可能造成模型学习的负担，并且很容易和其他标签重叠，因此必然需要进一步的修改标签和编码方式。","link":"/2023/05/02/W2NER/"},{"title":"hello world","text":"Some things about the blog Demo此处以Hexo创建GitHub Pages静态页面为例，作为后续技术总结的参考。 创建Github Repo: 默认已经注册有自己的Github账户了； 新建repository: 在仓库名字处，写上username.github.io； 这个username就是自己github注册的名字。例如自己进入自己github的主页时显示为xxx，则新建仓库时名字就为”xxx.github.io”； 配置Git SSH: 在自己电脑本地下载Git，比如Git for Windows； 在Git Bash里面输入： 12git config --global user.name \"your user name\"git config --global user.email \"your github email\" 之后: 123ssh-keygen -t rsa -C \"your github email\"# 之后按照提示操作即可，或者直接三个回车到底cd ~/.ssh 将.pub文件的内容复制好，进入github的settings页面，点击SSH and GPG keys，新增SSH keys，在key中复制刚刚剪切板的内容； 安装npm和Hexo: 在本地安装npm，如Node.js (nodejs.org) 进入Hexo官网，按照提示在本地希望后续保存博客的路径中，用Git Bash逐个输入： 12345npm install hexo-cli -ghexo init your_blog_namecd your_blog_namenpm installhexo server 之后，就可以在浏览器中输入：localhost:4000查看本地博客是否成功创建 配置Hexo: 在该博客路径中，打开_config.yml中修改各种信息，可以参考：配置 | Hexo 希望创建新的页面，比如about或者links等，输入: 1hexo new page \"about\" 之后在source/about/index.md中进行修改即可 希望创建新的文章，输入： 1hexo new \"hello world\" 之后在source/_post中修改对应名字的文章即可 部署Hexo: 要将博客部署在刚刚创建的xxx.github.io中，则打开本地博客的_config.yml，找到deploy处，修改为： 1234deploy: type: git repo: https://github.com/xxx/xxx.github.io branch: main 之后安装依赖，输入： 1npm install hexo-deployer-git --save 完成后，分别输入以下命令： 123hexo cleanhexo g # 产生静态文件hexo d # 自动部署到github.io页面 等待大概3分钟，即可在https://xxx.github.io里面看到自己的博客内容了。 启动latex渲染： 注意，保持原版不变即可，不需要再在网上查找各种复杂的教程安装各种包； 输入以下命令安装包： 1npm install hexo-filter-mathjax 进入博客的_config.yml，在末尾添加以下内容： 1234567891011mathjax: tags: none # or 'ams' or 'all' single_dollars: true # enable single dollar signs as in-line math delimiters cjk_width: 0.9 # relative CJK char width normal_width: 0.6 # relative normal (monospace) width append_css: true # add CSS to pages rendered by MathJax every_page: false # if true, every page will be rendered by MathJax regardless the `mathjax` setting in Front-matter packages: # extra packages to load extension_options: {} # you can put your extension options here # see http://docs.mathjax.org/en/latest/options/input/tex.html#tex-extension-options for more detail 之后，对于需要使用latex的文章，在其front-matter处增加mathjax: true，再重新生成和部署博客即可。 123456---title: Enable Latex on your articlecategories: Exampledate: 1905-06-30 12:00:00mathjax: true--- 公式示例： 行间公式： This is a in-line latex demo: 块间公式： 安装自定义字体： 如果需要引用google在线字体，可自定于网络搜索； 考虑到很多时候引用网络字体会出现不可预见的情况，因此可以将自己心仪字体的ttf下载下来； 假设下载的字体文件为abc.ttf； 在博客当前所使用的主题路径下:source/css，找到可能的字体文件样式表，如果没有，则自行创建。此次假设该文件叫：style.styl； 将字体文件放在样式路径下面，比如创建source/font目录，并将abc.ttf放在该font目录下； 在style.styl中，新增或修改： 12345678@font-face{ font-family: 'abc'; src: url('../font/abc.ttf')}body { font-family: 'abc';} 此处也可以推广，即url可以新增更多字体，也包括网络字体； body中指定的为网页字体渲染的顺序。 然而，在部署到远程服务器上时，有可能存在相对路径错误导致无法看到字体的情况，那么应该结合实际产生的静态文件的路径配置，自行修改src: url('../font/abc.ttf')的地址。 Hexo文章中插入图片： 使用markdown时，插入图片非常方便，但在早期的Hexo中，相关的操作并不友好； 在Hexo 3时代，我们可以通过非常简单的方法完成以前相对繁琐的操作； 首先，确保博客node_modules中有hexo-renderer-marked包，没有则安装： 1npm install hexo-renderer-marked --save 之后，在博客的_config.yml中修改和新增： 1234post_asset_folder: truemarked: prependRoot: true postAsset: true 之后使用命令新增文章时，会同时创建同名的资源文件夹，可以在里面放置图片等资源，然后在文章中使用markdown语法插入图片即可，并且只需要指定资源的名字，不用再指定路径，因为它默认在同名的资源文件中查找相关资源。 1![插入图片](image.jpg) Hexo总结： 如果需要更改默认主题，则在相关的主题网站查找Hexo主题，按照他们的说明进行下载和修改。一般来说主要经过下载–改名–放入博客theme路径–修改博客_config中的theme选项–修改主题文件的配置 这些流程； 如果需要新建分类： 1hexo new page categories 之后进入surce/categories/index.md并修改为： 12345---title: categoriesdate: 2023-05-01 13:47:40type: \"categories\"--- 后续新建文章后，只需要在文章标头中增加categories: 自定义类别名即可 如果需要新建标签： 同上，无非是将相关的categories改为tags即可。 如果需要对文章显示其摘要：由于大多数主题不支持自定义摘要，因此可以在希望显示的文字后面加上： 1&lt;!-- more --&gt; 本地服务器预览 1hexo s 新建文章 123456hexo n \"新文章\"# 新建文章hexo n page new_page# 新建页面hexo new page --path about/me \"About me\"# 指定路径新建页面 此外，文章还可以有很多属性，包括但不限于： layout ：page或者post title：文章标题 date：创建日期 updated：修改日期 comments ：是否开启评论，默认true tags：标签名 categories：分类名 产生静态文件（假设要部署在自己的服务器上，则需要手动移动静态文件) 1hexo g 清除缓存 1hexo clean 存储草稿 1hexo new draft \"new draft\" 该命令会生成source/_draft/new-draft.md，这些文章不会被发表，不会被链接查看到，可以当作自己撤销的、临时的或者私密的文章来用。 部署博客 123hexo dhexo clean &amp;&amp; hexo d# 一般可以用组合命令 后续若有新增，将会于此补充","link":"/2023/05/01/hello-world/"},{"title":"A Simple API And Backend for Chatbot Model","text":"Introduce a simple Flask API project code for large language model interaction. Background前段时间在我们自己的语料上微调大模型，在基本完成训练之后，我们需要将模型上线供他人使用，然而我还没正式工作过，更没系统地学习过后端开发，通过结合自己的编码经验和搜索引擎，初步完成了一份主要针对这类模型交互，或者以用户输入来获取信息的场景的后端系统，以API接口的方式对前端提供服务。 由于前端是另一名同学开发的，因此这里不做分享，但只要简单查看我所提供的代码，删除前端的同学也能非常快速地完成相关交互控件的编写。 项目目前已经开源：abel-nlp/model-call-backend-api Introduction项目结构: api.py 主要交互逻辑的文件，负责为前端提供api调用，以及对不同调用维护相应数据库； model.py 模型文件，为了避免每次api和数据库交互修改都要重新载入模型，而LLM每次载入的时间很长，因此将模型独立出来，形成自己的接口，一次部署，多次使用，极少下线； utils.py 一些功能函数，包括整理用户输入的函数、邮件地址验证和自动发送邮件的相关配置； ban.txt 违禁词的列表，每个违禁词为一行，避免用户随便问问题，恶意套取和截图模型的输出 基本流程：首先确保环境搭好，数据库的表建立完毕 运行model.py，将模型挂载到显存中，其中的api接口只负责接收输入，并返回模型的输出，此外不操作数据库，因此上线后只要不是改模型和接口参数，则后期维护都不需要下线； 运行api.py，后续只需要等待前端对不同功能的调用即可，有任何需要修改的地方，下线和重新部署也非常快。 主要功能： 单轮对话：用户每次的问题对模型而言都是新的问题，模型不需要上下文，只关心本次对话并回答，适合临时和偶尔询问模型； 多轮对话：用户在一次对话行为中，每次输入的对话对模型而言都是需要记住的； 敏感词检测：原因正如上面提到的那样； 用户注册和自动邮件发送：当用户提交注册后，相关的注册信息会先经过验证，如果没有注册过，则会生成随机的key作为邀请码，并自动地发送包含邀请码的邮件； 邀请码管理：检测邀请码是否存在，剩余使用次数和过期时间等； 恶意使用封禁：针对短期高频使用者(往往是爬虫或DDoS攻击)，会进行一定程度上的封禁； 免费用户管理：对于没有邀请码的用户，提供每天6次的免费体验，24小时后自动刷新 主要环境 python3 + Ubuntu + mysql8 pymysql flask DBUtils Details代码具体的细节可以在项目仓库里面查看，这里只概要地描述一些核心部分。 数据库的操作： 因为是以接口的形式提供，自然而然地将数据库连接当作全局变量来提供了，即： 12345678910111213pool = PooledDB( creator=pymysql, maxconnections=20, blocking=True, host='localhost' # 或服务器地址, port=3306, # 按实际端口输入，默认3306 user='xxx', password='xxxx', database='xxxx', charset='utf8mb4', ping=2 )conn = pool.connection() 然而，这种方式会强行将数据库缓存读到内存，之后即便显式地commit，也无法将修改和变动写入本地数据库，除非调用**conn.close()**，但这样需要在每个操作数据库的语句处都进行调用，而且还得重启连接，反而增加了数据库和代码编写的负担。 因此，将conn = pool.connection()去除，所有的数据库操作抽象出来，以函数的形式提供，这样即不需要重复地进行try except判断，也不需要反复进行冗余的连接与断开操作（因为函数内部都做了）； 当然，还可以控制mysql，强制不使用它的缓存，但对于我们这个小项目而言，这种方式并无不妥。具体来说，主要抽象了这五个函数： **get_conn()负责数据库连接；get_cursor()提供连接后的游标；close_conn()负责断开连接；有了这三个基本操作后，将数据库的查询类操作包装为query()函数，将更新和插入操作打包为insert()**函数，则后面再设计数据库操作时，只需要配合这两个函数就行。 接下来以注册为例来说明： 首先建立invite表： 123456789101112CREATE TABLE invite ( invite_key VARCHAR(36) PRIMARY KEY, ip_address VARCHAR(30), user_name VARCHAR(50), email VARCHAR(255), institution VARCHAR(255), apply_reason VARCHAR(255), register_time TIMESTAMP NOT NULL, expire_time TIMESTAMP NOT NULL, left_count INT DEFAULT 999 NOT NULL, is_activated BOOL DEFAULT FALSE NOT NULL); 该表的设计逻辑在于：invite_key是主键，邀请码，用于验证用户身份，其中expire_time和left_count共同控制用户使用，如果用户使用的时间已经超过过期时间，则无论剩下次数是多少都无法再使用，而在可用期间，只要剩余次数不为0，则可以继续使用，并且每次调用模型都会减少一次次数。 那么注册的代码就可以这样写： 1234567891011121314151617def register(ip_, name_, mail_, inst_, rea_): ip_query = &quot;SELECT COUNT(*) FROM invite WHERE ip_address=%s&quot; mail_query = &quot;SELECT COUNT(*) FROM invite WHERE email=%s&quot; if query(ip_query, ip_)[0] &gt;= 3 or query(mail_query, mail_)[0] &gt; 0: # 该email已经注册过 或者 ip注册过3次 return -1 else: key_ = generate_key() init_count = 500 time_delta = datetime.timedelta(days=30) sql = &quot;INSERT INTO invite (invite_key, ip_address, user_name, &quot; \\ &quot;email, institution, apply_reason, &quot; \\ &quot;register_time, expire_time, left_count, is_activated) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, FALSE)&quot; now = datetime.datetime.now() expire = now + time_delta insert(sql, key_, ip_, name_, mail_, inst_, rea_, now, expire, init_count) 上面代码删除了很多判断语句和与注册无关的语句（完整的可以看代码仓库）；可以看到，在具体的操作，例如以ip和email为条件来查询数据库，或者将邀请码等信息插入到invite表中，都可以用**query(sql, *args)或insert(sql, &amp;args)**的方式简便地操作数据库 当然，这种方式的缺点也很明显，就行对于上线后的场景，假设我作为用户拥有500次的调用权限，那么每次查询模型都需要直接操作数据库，当人数变多后，将会增加数据库的负担。因此这种只改变次数的操作可以先在缓存中操作，等到一定时间再一次性地写入本地数据库中。但因为我们面向的并不是大规模用户群体，主要目的还是实验和测试大模型，甚至还需要在后台操作数据库，且就只有两天时间开发，因此选择了这种方式。 邀请码产生 这个部分非常简单，可替代得方案也非常多，例如MD5加密等，我在这里选择了相对最简单的一种方式：随机数 只要随机数够长，就能尽可能避免邀请码重复，但光有数字肯定是不行的，因此这样来写： 12345678import stringimport secretsdef generate_key(): length = 36 chars = string.ascii_letters + string.digits key = ''.join(secrets.choice(chars) for _ in range(length)) return key secrets库是一个python的标准库模块，用于生成安全的随机数以及密码。使用该库可以生成高强度的、随机的数字、字母和符号组成的字符串，这些字符串可以用作密码、令牌等敏感信息的安全存储，这里先得到一个字符集合：数字和全部字母的集合，再让该库生成给定长度的组合 假设这次运行了该代码，得到了：eHz4qZziyTqoeYkrhVvORJ0wN4FO1aSgkcQB作为邀请码（随机的） 当然，还有其他简答方法，如直接调用**secrets.token_bytes(16)得到类似：b'\\x8d\\t\\xf1j\\xf8\\xae\\xae\\xe0\\x7f\\x897\\x06\\xe5\\xdd\\xe0|'的长16的随机字节序列；调用：secrets.token_hex(20)**得到长20的随机密码：6ad82b2535acab5afc14601096157fff4d318f77 自动邮件发送： 邮件发送主要使用了以下的库： 1234import reimport smtplibfrom email.header import Headerfrom email.mime.text import MIMEText 这个说起来比较复杂，所以直接说实现，这里的实现是以QQ邮箱为例的，而其他邮箱，如Gmail等也支持该操作，但是授权码、协议和端口则不一样； 在我们这个场景中，发送邮件的逻辑是： 用户注册，验证邮箱，产生验证码； 将用户注册的信息，转发到某个邮箱2上，这个转发的过程是由邮箱1承担的； 邮箱2由人工审核，决定是否将该邀请码发送给注册者； 那么这就有两个问题：邮箱1和2可不可以是同一个？以及每次都要由邮箱2人工审核是不是非常麻烦？对于第一个问题，是可以同一个的，自己给自己发没有问题；对于第二个问题，其中自动发送时，不转发给邮箱2，而是直接发送给注册者就行，但是在我们这里是为了，或者说不得不审核，所以才这样。那么怎么才能简化操作呢？ 这就用到了mailto机制，在html和markdown中有很多这种应用，就是自己点了一下某个邮箱，他就自动跳转到发邮件的地方了，甚至收件人和主题都写好了，那么这里也用了该方法，当邮箱2收到转发的消息后，只要同意，只需要点击连接，就自动生成了发送稿，包括注册用户的邮箱、回信和回复的邀请码。 具体实现可以直接看代码: 123456789101112131415161718192021222324252627282930313233343536373839404142receiver = &quot;xxx&quot; # 邮件2sender = 'xxx' # 邮件1mail_pass = 'xxxxx' # QQ邮箱授权码，不明白请自行查询smtp_server = 'smtp.qq.com'smtp_port = 465# 验证注册邮箱是否合格，其实这个应该是前端在输入框中限制的，因此这里只是写出来而已def validate_email(mail_): pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$' if re.match(pattern, mail_): return True else: return False def send_mail(user_, mail_, rea_, key_, rec_=receiver): &quot;&quot;&quot; :param user_: 注册用户的名字 :param rec_: 决定是否发消息的人 :param mail_: 注册用户的email :param rea_: 注册用户的申请理由 :param key_: 自动生成的key :return: Bool &quot;&quot;&quot; approve = f&quot;&lt;a href='mailto:{mail_}?subject=TechGPT申请通过&amp;body={user_}您好，&quot; \\ f&quot;您的TechGPT申请已经通过，这是您的邀请码：{key_}&gt;&quot; body_ = f&quot;您好，我是{user_}. \\n我申请的理由是：{rea_}, 如果您同意，请直接点击下面链接发送消息: \\n{approve}&quot; msg = MIMEText(body_, 'plain', 'utf-8') msg['Subject'] = Header(f&quot;{user_} 申请使用TechGPT&quot;, 'utf-8') msg['From'] = sender msg['To'] = Header(rec_, 'utf-8') try: smtp = smtplib.SMTP_SSL(smtp_server, smtp_port) smtp.login(sender, mail_pass) smtp.sendmail(sender, rec_, msg.as_string()) except smtplib.SMTPException as e: print('邮件发送错误: ', e) return False return True 上面的body就是利用mailto方式完成的邮件主体。当然了，还可以使用类似激活邮件的方式完全自动审核，也是非常容易的，可以自行设计，真的非常简单！ 注意：不要用自己的QQ邮箱正式使用，测试可以用，最好申请临时的，或者专用的，避免因大量转发邮件被封号。 其他：除了上面的，还有很多操作，但都比较简单，这里只简述一下flask接口 一般只需要flask包就完成了，不过我们这里涉及用一个接口调另一个接口（api.py调model.py），因此还需要用到requests包，当然也都非常简单； 假设我们需要一个接口，端口号为8081，其调用的api名字为hello，我们可以简短地实现： 当然，如果api的部署不在本地，最好确认端口已经打开了，不放心可以（以Ubuntu为例）: 12sudo firewall-cmd --zone=public --add-port=8081/tcp --permanentsudo firewall-cmd --reload 123456789from flask import Flask, requestapp = Flask(__name__)@app.route('/hello', methods=['POST'])def hello_api(): return {&quot;api_status&quot;: &quot;hello world!&quot;}if __name__ == '__main__': app.run(host='0.0.0.0', port=8081) 这样一旦运行起来，那么通过构造请求就可以得到返回，我们以postman发送请求为例： 当然，这样比较过于简单，因此我们假设在请求时需要传递参数，参数为：text1和text2，并希望api返回他俩的组合，中间以空格隔开，那么修改上面的hello_api函数： 1234567@app.route('/hello', methods=['POST'])def hello_api(): json_data = request.get_json() text1 = json_data['text1'] text2 = json_data['text2'] text = text1 + ' ' + text2 return {&quot;concat&quot;: text} postman再构造一下两个文本为：Hello和World，结果为： 除了这种调用方式，还可以利用requests进行调用，我们这里以 api.py收到信息后，将参数转发给model.py的操作为例: 其中url是model.py的服务地址、端口和接口名，json构造的就类似前面text1一样的参数，发送过去后，如果model.py能正常返回，则状态码为200，并得到json数据，再用key从json数据中取值即可。 以上为草稿版","link":"/2023/06/14/A-Simple-API-And-Backend-for-Chatbot-Model/"}],"tags":[{"name":"tech-notes","slug":"tech-notes","link":"/tags/tech-notes/"},{"name":"todo-list","slug":"todo-list","link":"/tags/todo-list/"},{"name":"paper-notes-collection","slug":"paper-notes-collection","link":"/tags/paper-notes-collection/"},{"name":"paper-notes","slug":"paper-notes","link":"/tags/paper-notes/"},{"name":"official","slug":"official","link":"/tags/official/"}],"categories":[]}